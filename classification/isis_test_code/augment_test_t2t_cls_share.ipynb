{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e2be86-eab5-4f9e-8133-677609bf30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from my_dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
    "from my_dassl.config import get_cfg_default\n",
    "from my_dassl.engine import build_trainer\n",
    "\n",
    "import datasets.oxford_pets\n",
    "import datasets.oxford_flowers\n",
    "import datasets.fgvc_aircraft\n",
    "import datasets.dtd\n",
    "import datasets.eurosat\n",
    "import datasets.stanford_cars\n",
    "import datasets.food101\n",
    "import datasets.sun397\n",
    "import datasets.caltech101\n",
    "import datasets.ucf101\n",
    "import datasets.imagenet\n",
    "import datasets.svhn\n",
    "import datasets.resisc45\n",
    "import datasets.clevr\n",
    "\n",
    "import datasets.locmnist\n",
    "import datasets.colour_biased_mnist\n",
    "\n",
    "import trainers.coop\n",
    "import trainers.cocoop\n",
    "import trainers.zsclip\n",
    "import trainers.ftclip\n",
    "import trainers.vpwb\n",
    "import trainers.vpour\n",
    "# import trainers.blackvip\n",
    "import trainers.blackvip_t2t\n",
    "# import trainers.blackvip_jh\n",
    "import trainers.reprogramming\n",
    "\n",
    "import pdb\n",
    "\n",
    "PATCH_NUM = 0\n",
    "DATA = './datasets'\n",
    "TRAINER = 'BLACKVIP'\n",
    "SHOTS = 16\n",
    "CFG = 'vit_b16'\n",
    "ptb = 'vit-mae-base'\n",
    "\n",
    "DATASET = 'eurosat'\n",
    "ep = 5000\n",
    "\n",
    "spsa_os = 1.0\n",
    "alpha = 0.4\n",
    "spsa_a = 0.01\n",
    "\n",
    "b1 = float(0.9)\n",
    "gamma = float(0.2)\n",
    "spsa_c = float(0.005)\n",
    "p_eps = float(0.4)\n",
    "\n",
    "opt_type = 'spsa-gc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e84b1f7-cfc2-45ad-880f-5639a755e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Add new config variables.\n",
    "\n",
    "    E.g.\n",
    "        from yacs.config import CfgNode as CN\n",
    "        cfg.TRAINER.MY_MODEL = CN()\n",
    "        cfg.TRAINER.MY_MODEL.PARAM_A = 1.\n",
    "        cfg.TRAINER.MY_MODEL.PARAM_B = 0.5\n",
    "        cfg.TRAINER.MY_MODEL.PARAM_C = False\n",
    "    \"\"\"\n",
    "    from yacs.config import CfgNode as CN\n",
    "\n",
    "    # ! DATASET CONFIG\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"  # all, base or new\n",
    "\n",
    "    cfg.DATASET.LOCMNIST = CN()\n",
    "    cfg.DATASET.LOCMNIST.R_SIZE = 1\n",
    "    cfg.DATASET.LOCMNIST.F_SIZE = 4\n",
    "\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST = CN()\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.TRAIN_RHO = 0.8\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.TEST_RHO = 0.2\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.TRAIN_N_CONFUSING_LABELS = 9\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.TEST_N_CONFUSING_LABELS = 9\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.USE_TEST_AS_VAL = True\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.RANDOMIZE = True if args.randomize else False\n",
    "\n",
    "    # ! Bahng et al. Visual Prompting (VP)\n",
    "    cfg.TRAINER.VPWB = CN()\n",
    "    cfg.TRAINER.VPWB.PREC = \"amp\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.VPWB.METHOD = 'padding'  # 'padding', 'fixed_patch', 'random_patch'\n",
    "    cfg.TRAINER.VPWB.IMAGE_SIZE = 224\n",
    "    cfg.TRAINER.VPWB.PROMPT_SIZE = 30\n",
    "\n",
    "    # ! Visual Prompting (VP) with SPSA\n",
    "    cfg.TRAINER.VPOUR = CN()\n",
    "    cfg.TRAINER.VPOUR.METHOD = 'padding'\n",
    "    cfg.TRAINER.VPOUR.IMAGE_SIZE = 224\n",
    "    cfg.TRAINER.VPOUR.PROMPT_SIZE = 30\n",
    "    cfg.TRAINER.VPOUR.SPSA_PARAMS = [0.0, 0.001, 40.0, 0.6, 0.1]\n",
    "    cfg.TRAINER.VPOUR.OPT_TYPE = \"spsa-gc\"\n",
    "    cfg.TRAINER.VPOUR.MOMS = 0.9\n",
    "    cfg.TRAINER.VPOUR.SP_AVG = 5\n",
    "\n",
    "    # ! BlackVIP\n",
    "    cfg.TRAINER.BLACKVIP = CN()\n",
    "    cfg.TRAINER.BLACKVIP.METHOD = 'coordinator'\n",
    "    cfg.TRAINER.BLACKVIP.PT_BACKBONE = ptb  # vit-base / vit-mae-base\n",
    "    cfg.TRAINER.BLACKVIP.SRC_DIM = 1568  # 784 / 1568 / 3136 #? => only for pre-trained Enc\n",
    "    cfg.TRAINER.BLACKVIP.E_OUT_DIM = 0  # 64 / 128 / 256 #? => only for scratch Enc\n",
    "    cfg.TRAINER.BLACKVIP.SPSA_PARAMS = [1.0, 0.005, 0.01, 0.4, 0.2]\n",
    "    cfg.TRAINER.BLACKVIP.OPT_TYPE = \"spsa-gc\"  # [spsa, spsa-gc, naive]\n",
    "    cfg.TRAINER.BLACKVIP.MOMS = b1  # first moment scale.\n",
    "    cfg.TRAINER.BLACKVIP.SP_AVG = 5  # grad estimates averaging steps\n",
    "    cfg.TRAINER.BLACKVIP.P_EPS = p_eps  # prompt scale\n",
    "    cfg.TRAINER.BLACKVIP.smoothing = False\n",
    "    # ! Black-Box Adversarial Reprogramming (BAR)\n",
    "    cfg.TRAINER.BAR = CN()\n",
    "    cfg.TRAINER.BAR.METHOD = 'reprogramming'\n",
    "    cfg.TRAINER.BAR.LRS = [0.01, 0.0001]\n",
    "    cfg.TRAINER.BAR.FRAME_SIZE = 224\n",
    "    cfg.TRAINER.BAR.SMOOTH = 0.01\n",
    "    cfg.TRAINER.BAR.SIMGA = 1.0\n",
    "    cfg.TRAINER.BAR.SP_AVG = 5\n",
    "    cfg.TRAINER.BAR.FOCAL_G = 2.0\n",
    "\n",
    "    # ! Full Fine Tune / Linear Probe\n",
    "    cfg.TRAINER.FTCLIP = CN()\n",
    "    cfg.TRAINER.FTCLIP.METHOD = 'ft'  # 'ft', 'lp'\n",
    "\n",
    "    # ! CoOp, CoCoOp\n",
    "    cfg.TRAINER.COOP = CN()\n",
    "    cfg.TRAINER.COOP.N_CTX = 16  # number of context vectors\n",
    "    cfg.TRAINER.COOP.CSC = False  # class-specific context\n",
    "    cfg.TRAINER.COOP.CTX_INIT = \"\"  # initialization words\n",
    "    cfg.TRAINER.COOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.COOP.CLASS_TOKEN_POSITION = \"end\"  # 'middle' or 'end' or 'front'\n",
    "\n",
    "    #\n",
    "    cfg.OPTIM.MAX_EPOCH = 500\n",
    "    cfg.DATASET.NUM_SHOTS = 16\n",
    "    cfg.TRAIN.CHECKPOINT_FREQ = 500\n",
    "    cfg.TRAINER.BLACKVIP.P_EPS_number = PATCH_NUM\n",
    "\n",
    "    cfg.TRAINER.COCOOP = CN()\n",
    "    cfg.TRAINER.COCOOP.N_CTX = 16  # number of context vectors\n",
    "    cfg.TRAINER.COCOOP.CTX_INIT = \"\"  # initialization words\n",
    "    cfg.TRAINER.COCOOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "def reset_cfg(cfg, args):\n",
    "    if args.root:\n",
    "        cfg.DATASET.ROOT = args.root\n",
    "\n",
    "    if args.eval_only:\n",
    "        cfg.eval_only = 1\n",
    "    else:\n",
    "        cfg.eval_only = 0\n",
    "\n",
    "    if args.output_dir:\n",
    "        cfg.OUTPUT_DIR = args.output_dir\n",
    "\n",
    "    if args.resume:\n",
    "        cfg.RESUME = args.resume\n",
    "\n",
    "    if args.seed:\n",
    "        cfg.SEED = args.seed\n",
    "\n",
    "    if args.wb_method_name != 'no':\n",
    "        cfg.WB_METHOD_NAME = args.wb_method_name\n",
    "\n",
    "    if args.use_wandb:\n",
    "        cfg.use_wandb = 1\n",
    "    else:\n",
    "        cfg.use_wandb = 0\n",
    "\n",
    "    cfg.EVAL_MODE = 'best'\n",
    "\n",
    "    if args.source_domains:\n",
    "        cfg.DATASET.SOURCE_DOMAINS = args.source_domains\n",
    "\n",
    "    if args.target_domains:\n",
    "        cfg.DATASET.TARGET_DOMAINS = args.target_domains\n",
    "\n",
    "    if args.transforms:\n",
    "        cfg.INPUT.TRANSFORMS = args.transforms\n",
    "\n",
    "    if args.trainer:\n",
    "        cfg.TRAINER.NAME = args.trainer\n",
    "\n",
    "    if args.backbone:\n",
    "        cfg.MODEL.BACKBONE.NAME = args.backbone\n",
    "\n",
    "    if args.head:\n",
    "        cfg.MODEL.HEAD.NAME = args.head\n",
    "    if args.patch_size:\n",
    "        cfg.TRAINER.PATCH = args.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fdfa39-fce6-4726-bc99-2b30ecaa6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(args):\n",
    "    cfg = get_cfg_default()\n",
    "    extend_cfg(cfg)\n",
    "\n",
    "    # 1. From the dataset config file\n",
    "    if args.dataset_config_file:\n",
    "        cfg.merge_from_file(args.dataset_config_file)\n",
    "\n",
    "    # 2. From the method config file\n",
    "    if args.config_file:\n",
    "        cfg.merge_from_file(args.config_file)\n",
    "\n",
    "    # 3. From input arguments\n",
    "    reset_cfg(cfg, args)\n",
    "\n",
    "    # 4. From optional input arguments\n",
    "    # print(args.opts)\n",
    "    # cfg.merge_from_list(args.opts)\n",
    "\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "    \n",
    "import argparse\n",
    "\n",
    "# Manually creating a Namespace object to simulate command-line arguments\n",
    "args = argparse.Namespace(\n",
    "    root='./datasets',\n",
    "    output_dir=f'output/eurosat/BLACKVIP_right_t2t_cls/vit-mae-base_vit_b16_t2t_s_inter/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1',\n",
    "    # output_dir=f'output/eurosat/BLACKVIP/vit-mae-base_vit_b16/shot16_ep5000_orignal/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1',\n",
    "    # output_dir = f'output/eurosat/BLACKVIP/vit-mae-base_vit_b16/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4_jh/seed1',\n",
    "    # output_dir = f'output/eurosat/BLACKVIP/vit-mae-base_vit_b16_nt/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed2',\n",
    "    # output_dir = f'output/eurosat/BLACKVIP_right_t2t/vit-mae-base_vit_b16_t2t/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1',\n",
    "    # output_dir=f'output/colour_biased_mnist_easy/BLACKVIP/vit-mae-base_vit_b16_noaug/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.1_sa0.01_sc0.01_eps1.0/seed1',\n",
    "    resume='',\n",
    "    seed=1,\n",
    "    source_domains=None,  # Update this as needed\n",
    "    target_domains=None,  # Update this as needed\n",
    "    transforms=None,  # Update this as needed\n",
    "    config_file='configs/trainers/BLACKVIP/vit_b16_t2t_s_inter.yaml',\n",
    "    dataset_config_file='configs/datasets/eurosat.yaml',\n",
    "    trainer='BLACKVIP',\n",
    "    backbone='',  # Update this as needed\n",
    "    head='',  # Update this as needed\n",
    "    eval_only=False,\n",
    "    model_dir='',  # Update this as needed\n",
    "    load_epoch=None,  # Update this as needed\n",
    "    no_train=False,\n",
    "    use_wandb=False,\n",
    "    wb_name='test',\n",
    "    wb_method_name='no',\n",
    "    randomize=1,\n",
    "    patch_size = 16\n",
    "\n",
    "    # opts=None  # Update this as needed, if you need to pass any additional command line arguments\n",
    ")\n",
    "def print_args(args, cfg):\n",
    "    print(\"***************\")\n",
    "    print(\"** Arguments **\")\n",
    "    print(\"***************\")\n",
    "    optkeys = list(args.__dict__.keys())\n",
    "    optkeys.sort()\n",
    "    for key in optkeys:\n",
    "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
    "    print(\"************\")\n",
    "    print(\"** Config **\")\n",
    "    print(\"************\")\n",
    "    print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b616348c-9cd2-4cde-858d-02d8c8c27626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/BLACKVIP/vit_b16_t2t_s_inter.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "output_dir: output/eurosat/BLACKVIP_right_t2t_cls/vit-mae-base_vit_b16_t2t_s_inter/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1\n",
      "patch_size: 16\n",
      "randomize: 1\n",
      "resume: \n",
      "root: ./datasets\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: BLACKVIP\n",
      "transforms: None\n",
      "use_wandb: False\n",
      "wb_method_name: no\n",
      "wb_name: test\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 32\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 128\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  COLOUR_BIASED_MNIST:\n",
      "    RANDOMIZE: True\n",
      "    TEST_N_CONFUSING_LABELS: 9\n",
      "    TEST_RHO: 0.2\n",
      "    TRAIN_N_CONFUSING_LABELS: 9\n",
      "    TRAIN_RHO: 0.8\n",
      "    USE_TEST_AS_VAL: True\n",
      "  LOCMNIST:\n",
      "    F_SIZE: 4\n",
      "    R_SIZE: 1\n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: ./datasets\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "EVAL_MODE: best\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.5\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 5000\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 100\n",
      "  WARMUP_MIN_LR: 0.001\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: linear\n",
      "  WEIGHT_DECAY: 0.0\n",
      "OUTPUT_DIR: output/eurosat/BLACKVIP_right_t2t_cls/vit-mae-base_vit_b16_t2t_s_inter/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 500\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  BAR:\n",
      "    FOCAL_G: 2.0\n",
      "    FRAME_SIZE: 224\n",
      "    LRS: [0.01, 0.0001]\n",
      "    METHOD: reprogramming\n",
      "    SIMGA: 1.0\n",
      "    SMOOTH: 0.01\n",
      "    SP_AVG: 5\n",
      "  BLACKVIP:\n",
      "    E_OUT_DIM: 0\n",
      "    METHOD: coordinator\n",
      "    MOMS: 0.9\n",
      "    OPT_TYPE: spsa-gc\n",
      "    PT_BACKBONE: vit-prompt_share_inter_5\n",
      "    P_EPS: 0.4\n",
      "    P_EPS_number: 0\n",
      "    SPSA_PARAMS: [1.0, 0.01, 0.01, 0.4, 0.1]\n",
      "    SP_AVG: 5\n",
      "    SRC_DIM: 1568\n",
      "    smoothing: False\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: end\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  FTCLIP:\n",
      "    METHOD: ft\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: BLACKVIP\n",
      "  PATCH: 16\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "  VPOUR:\n",
      "    IMAGE_SIZE: 224\n",
      "    METHOD: padding\n",
      "    MOMS: 0.9\n",
      "    OPT_TYPE: spsa-gc\n",
      "    PROMPT_SIZE: 30\n",
      "    SPSA_PARAMS: [0.0, 0.001, 40.0, 0.6, 0.1]\n",
      "    SP_AVG: 5\n",
      "  VPWB:\n",
      "    IMAGE_SIZE: 224\n",
      "    METHOD: padding\n",
      "    PREC: amp\n",
      "    PROMPT_SIZE: 30\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "eval_only: 0\n",
      "use_wandb: 0\n",
      "Collecting env info ...\n",
      "Loading trainer: BLACKVIP\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /home/connor/BlackVIP/datasets/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /home/connor/BlackVIP/datasets/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  160\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Text Prompts: ['a centered satellite photo of Annual Crop Land.', 'a centered satellite photo of Forest.', 'a centered satellite photo of Herbaceous Vegetation Land.', 'a centered satellite photo of Highway or Road.', 'a centered satellite photo of Industrial Buildings.', 'a centered satellite photo of Pasture Land.', 'a centered satellite photo of Permanent Crop Land.', 'a centered satellite photo of Residential Buildings.', 'a centered satellite photo of River.', 'a centered satellite photo of Sea or Lake.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing PromptViTForImageClassification: ['decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.mask_token', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.0.output.dense.weight']\n",
      "- This IS expected if you are initializing PromptViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PromptViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PromptViTForImageClassification were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.bias', 'vit.deep_prompt_embeddings', 'vit.pooler.dense.weight', 'vit.prompt_embeddings']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/connor/.conda/envs/bl/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading evaluator: Classification\n",
      "Load output/eurosat/BLACKVIP_right_t2t_cls/vit-mae-base_vit_b16_t2t_s_inter/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1/coordinator/model.pth.tar-5000 to coordinator (epoch=5000)\n"
     ]
    }
   ],
   "source": [
    "args.dataset_config_file = f'configs/datasets/eurosat.yaml'\n",
    "cfg = setup_cfg(args)\n",
    "if cfg.SEED >= 0:\n",
    "    print(\"Setting fixed seed: {}\".format(cfg.SEED))\n",
    "    set_random_seed(cfg.SEED)\n",
    "print_args(args, cfg)\n",
    "print(\"Collecting env info ...\")\n",
    "\n",
    "# for strage test\n",
    "AVAI_CHOICES = [\"random_flip\", \"random_resized_crop\", \"normalize\", \"instance_norm\", \"random_crop\", \"random_translation\", \"center_crop\",\n",
    "    \"cutout\", \"imagenet_policy\", \"cifar10_policy\", \"svhn_policy\", \"randaugment\", \"randaugment_fixmatch\", \"randaugment2\", \"gaussian_noise\",\n",
    "    \"colorjitter\", \"randomgrayscale\", \"gaussian_blur\",]\n",
    "new_test_type = None #['randaugment_fixmatch'] # None # ['randaugment_fixmatch'] # None\n",
    "# data suffle\n",
    "cfg.merge_from_list(['DATALOADER.TEST.SAMPLER', 'RandomSampler']) # 'RandomSampler', 'SequentialSampler'\n",
    "# for other data\n",
    "cfg.merge_from_file(args.dataset_config_file)\n",
    "\n",
    "if new_test_type is None:\n",
    "    pass\n",
    "elif new_test_type[0] == 'base':\n",
    "    cfg.merge_from_list(['INPUT.TRANSFORMS', ('normalize', 'new_test_type')])\n",
    "else:\n",
    "    cfg.merge_from_list(['INPUT.TRANSFORMS', ('normalize', *new_test_type, 'new_test_type')])\n",
    "\n",
    "# Creating the tuple with the desired format\n",
    "trainer = build_trainer(cfg)\n",
    "trainer.load_model(trainer.output_dir, epoch=5000)\n",
    "dtype = trainer.model.dtype\n",
    "data_loader =trainer.test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c194be76-7b56-4ec0-a374-0ee643e6579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                                | 150/254 [02:41<01:52,  1.08s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def clip_clipping(x):\n",
    "    #! -inf ~ inf -> CLIP's input RGB range\n",
    "    if len(x.shape) == 3:\n",
    "        out = torch.cat([torch.clip(x[0,:,:], min=-1.79226253, max=1.93033625).unsqueeze(0),\n",
    "                     torch.clip(x[1,:,:], min=-1.75209713, max=2.07488384).unsqueeze(0),\n",
    "                     torch.clip(x[2,:,:], min=-1.48021977, max=2.14589699).unsqueeze(0)], dim=0)\n",
    "    else:\n",
    "        out = torch.cat([torch.clip(x[:,0,:,:], min=-1.79226253, max=1.93033625).unsqueeze(1),\n",
    "                        torch.clip(x[:,1,:,:], min=-1.75209713, max=2.07488384).unsqueeze(1),\n",
    "                        torch.clip(x[:,2,:,:], min=-1.48021977, max=2.14589699).unsqueeze(1)], dim=1)\n",
    "    return out\n",
    "trainer.set_model_mode(\"eval\")\n",
    "trainer.evaluator.reset()\n",
    "data_loader =trainer.test_loader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "cnt = 0\n",
    "for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "    input, label = trainer.parse_batch_test(batch)\n",
    "    # if label[0] == 8:\n",
    "    # if not torch.all(label == label[0]):\n",
    "    #     continue\n",
    "    cnt +=1\n",
    "    prompt, z = trainer.model.coordinator(input.cuda().type(dtype))\n",
    "    prompt = clip_clipping(prompt)\n",
    "    # Flatten the images to shape [32, 150528]\n",
    "    flattened_images = prompt.cpu().view(32, -1)\n",
    "    flattened_images[1] = flattened_images[1]\n",
    "    # Compute the cosine similarity\n",
    "    cos_sim_matrix = torch.nn.functional.cosine_similarity(flattened_images[:, None], flattened_images, dim=-1)\n",
    "    mask = np.triu_indices_from(cos_sim_matrix, k=1)\n",
    "    if not batch_idx:\n",
    "        cos_sim_values = cos_sim_matrix[mask]\n",
    "    else:\n",
    "        cos_sim_values = torch.cat((cos_sim_values, cos_sim_matrix[mask]), dim=0)\n",
    "    if cnt>150:\n",
    "        break\n",
    "# Step 3 remains the same: Plot the distribution of cosine similarity values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cos_sim_values, bins=30, kde=0)\n",
    "try: \n",
    "    plt.title(f'Distribution of Cosine Similarities_{new_test_type[0]} (Excluding Self-Comparison)')\n",
    "except TypeError:\n",
    "    pass\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0.88, 0.99)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c381b6-783c-48ae-b5ad-a6b8208131dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9846) tensor(0.0103)\n"
     ]
    }
   ],
   "source": [
    "print( torch.mean(cos_sim_values),  torch.std(cos_sim_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc369523-41e1-4cce-8573-b6aa21ecbbb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2862b-53fe-47dc-ad04-3d9dbe8c5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from my_dassl.data.transforms import INTERPOLATION_MODES, build_transform\n",
    "import torchvision.transforms as T\n",
    "interp_mode = INTERPOLATION_MODES[cfg.INPUT.INTERPOLATION]\n",
    "to_tensor = []\n",
    "to_tensor += [T.Resize(cfg.INPUT.SIZE, interpolation=interp_mode)]\n",
    "to_tensor += [T.ToTensor()]\n",
    "if \"normalize\" in cfg.INPUT.TRANSFORMS:\n",
    "    normalize = T.Normalize(\n",
    "        mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD\n",
    "    )\n",
    "    to_tensor += [normalize]\n",
    "tensor_tr = T.Compose(to_tensor)\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define function to extract patches and calculate norms\n",
    "def extract_patches_and_calculate_norms(tensor, patch_size=4, threshold=150, check=False):\n",
    "    patches = tensor.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.contiguous().view(1, 3, -1, patch_size, patch_size)\n",
    "    patches = patches.permute(2, 0, 1, 3, 4).reshape(-1, 3, patch_size, patch_size)\n",
    "    norm_img = torch.norm(patches, p=2, dim=(1, 2, 3))\n",
    "    indices_under_t = norm_img < threshold\n",
    "    norms_under_t = norm_img[indices_under_t]\n",
    "    if check:\n",
    "        return norms_under_t, indices_under_t, norm_img\n",
    "    return norms_under_t, indices_under_t\n",
    "\n",
    "# Function to plot norms under threshold\n",
    "def plot_norms_under_threshold(ax, norms, indices, title, patch_size=4):\n",
    "    norms_image = torch.full((224 // patch_size, 224 // patch_size), float('nan'))\n",
    "    norms_image[indices.view(224 // patch_size, 224 // patch_size)] = norms\n",
    "    cax = ax.imshow(norms_image.numpy(), cmap='viridis')\n",
    "    fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "input_path = 'datasets/eurosat/2750/River/*.jpg'\n",
    "# input_path = 'colour_biased_mnist/images_rho=0.2_ncl=9_rand=True/test/9/*.png'\n",
    "eps = [1, 0.4]\n",
    "patch_size = 4\n",
    "threshold = 150\n",
    "prompt_max = 150\n",
    "path = f'check_prompt/{eps}_{patch_size}_{threshold}_{input_path[0:5]}_t2t_cls_check_detail_shared_{prompt_max}'\n",
    "os.makedirs(path,exist_ok=True)\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    trainer.set_model_mode(\"eval\")\n",
    "    trainer.evaluator.reset()\n",
    "    for i in glob(input_path):\n",
    "        ori_img = Image.open(i).convert(\"RGB\")\n",
    "        img = tensor_tr(ori_img)\n",
    "        img = img.unsqueeze(0)\n",
    "        prompt, _ = trainer.model.coordinator(img.cuda().type(dtype))\n",
    "        \n",
    "        # Create image tensor (assuming the image size is [1, 3, 224, 224])\n",
    "        \n",
    "\n",
    "        input_tensor_2 = clip_clipping(img.cpu() + prompt.cpu() * eps[1])\n",
    "        # Process both input and prompt tensors\n",
    "        norms_img, img_indices = extract_patches_and_calculate_norms(clip_clipping(img), patch_size, threshold)\n",
    "        norms_prompt, prompt_indices, prompt_norm = extract_patches_and_calculate_norms(clip_clipping(prompt.cpu()), patch_size, threshold, check=True)\n",
    "        norms_input_2, input_2_indices = extract_patches_and_calculate_norms(input_tensor_2, patch_size, threshold)\n",
    "        norms_input_max, input_max_indices = extract_patches_and_calculate_norms(clip_clipping(prompt.cpu()), patch_size, prompt_max)\n",
    "        \n",
    "        # Setup plot\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "        \n",
    "        # Original image\n",
    "        axs[0, 0].imshow(ori_img.resize((224, 224)))\n",
    "        axs[0, 0].set_title('Original Image')\n",
    "        \n",
    "        # Plot norms under threshold for input and prompt\n",
    "        plot_norms_under_threshold(axs[0, 1], norms_img, img_indices, f'Input Norms')\n",
    "        plot_norms_under_threshold(axs[1, 0], norms_prompt, prompt_indices, f'Prompt Norms')\n",
    "        plot_norms_under_threshold(axs[1, 2], norms_input_2, input_2_indices, f'Prompt Norms, eps : {eps[1]}')\n",
    "        plot_norms_under_threshold(axs[1, 1], norms_input_max, input_max_indices, f'Prompt Norms, eps : {eps[1]} < {prompt_max}')\n",
    "        # Distribution of all norms (Histogram)\n",
    "        axs[0, 2].hist(norms_prompt.numpy(), bins=30, alpha=0.7, log=True)\n",
    "        axs[0, 2].set_xlabel('Norm Value')\n",
    "        axs[0, 2].set_ylabel('Frequency (Log Scale)')\n",
    "        axs[0, 2].set_title('Distribution of All Norms')\n",
    "                \n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f'{path}/{os.path.basename(i)}')\n",
    "        plt.show()\n",
    "        cnt +=1\n",
    "        if cnt == 10:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972e18e-cfec-40b8-b787-0a12f486ccba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bl",
   "language": "python",
   "name": "bl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
