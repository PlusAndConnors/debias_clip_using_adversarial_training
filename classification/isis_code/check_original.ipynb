{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce299f9-3e51-46a7-8403-98b84e23a1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from my_dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
    "from my_dassl.config import get_cfg_default\n",
    "from my_dassl.engine import build_trainer\n",
    "\n",
    "import datasets.oxford_pets\n",
    "import datasets.oxford_flowers\n",
    "import datasets.fgvc_aircraft\n",
    "import datasets.dtd\n",
    "import datasets.eurosat\n",
    "import datasets.stanford_cars\n",
    "import datasets.food101\n",
    "import datasets.sun397\n",
    "import datasets.caltech101\n",
    "import datasets.ucf101\n",
    "import datasets.imagenet\n",
    "import datasets.svhn\n",
    "import datasets.resisc45\n",
    "import datasets.clevr\n",
    "\n",
    "import datasets.locmnist\n",
    "import datasets.colour_biased_mnist\n",
    "\n",
    "import trainers.coop\n",
    "import trainers.cocoop\n",
    "import trainers.zsclip\n",
    "import trainers.ftclip\n",
    "import trainers.vpwb\n",
    "import trainers.vpour\n",
    "import trainers.blackvip\n",
    "# import trainers.blackvip_t2t\n",
    "# import trainers.blackvip_jh\n",
    "import trainers.reprogramming\n",
    "\n",
    "import pdb\n",
    "\n",
    "PATCH_NUM = 0\n",
    "DATA = './datasets'\n",
    "TRAINER = 'BLACKVIP'\n",
    "SHOTS = 16\n",
    "CFG = 'vit_b16'\n",
    "ptb = 'vit-mae-base'\n",
    "\n",
    "DATASET = 'eurosat'\n",
    "ep = 5000\n",
    "\n",
    "spsa_os = 1.0\n",
    "alpha = 0.4\n",
    "spsa_a = 0.01\n",
    "\n",
    "b1 = float(0.9)\n",
    "gamma = float(0.2)\n",
    "spsa_c = float(0.005)\n",
    "p_eps = float(0.4)\n",
    "\n",
    "opt_type = 'spsa-gc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63060f0-d4dc-4a3a-ba6b-d7b10326ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Add new config variables.\n",
    "\n",
    "    E.g.\n",
    "        from yacs.config import CfgNode as CN\n",
    "        cfg.TRAINER.MY_MODEL = CN()\n",
    "        cfg.TRAINER.MY_MODEL.PARAM_A = 1.\n",
    "        cfg.TRAINER.MY_MODEL.PARAM_B = 0.5\n",
    "        cfg.TRAINER.MY_MODEL.PARAM_C = False\n",
    "    \"\"\"\n",
    "    from yacs.config import CfgNode as CN\n",
    "\n",
    "    # ! DATASET CONFIG\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"  # all, base or new\n",
    "\n",
    "    cfg.DATASET.LOCMNIST = CN()\n",
    "    cfg.DATASET.LOCMNIST.R_SIZE = 1\n",
    "    cfg.DATASET.LOCMNIST.F_SIZE = 4\n",
    "\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST = CN()\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.TRAIN_RHO = 0.8\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.TEST_RHO = 0.2\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.TRAIN_N_CONFUSING_LABELS = 9\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.TEST_N_CONFUSING_LABELS = 9\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.USE_TEST_AS_VAL = True\n",
    "    cfg.DATASET.COLOUR_BIASED_MNIST.RANDOMIZE = True if args.randomize else False\n",
    "\n",
    "    # ! Bahng et al. Visual Prompting (VP)\n",
    "    cfg.TRAINER.VPWB = CN()\n",
    "    cfg.TRAINER.VPWB.PREC = \"amp\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.VPWB.METHOD = 'padding'  # 'padding', 'fixed_patch', 'random_patch'\n",
    "    cfg.TRAINER.VPWB.IMAGE_SIZE = 224\n",
    "    cfg.TRAINER.VPWB.PROMPT_SIZE = 30\n",
    "\n",
    "    # ! Visual Prompting (VP) with SPSA\n",
    "    cfg.TRAINER.VPOUR = CN()\n",
    "    cfg.TRAINER.VPOUR.METHOD = 'padding'\n",
    "    cfg.TRAINER.VPOUR.IMAGE_SIZE = 224\n",
    "    cfg.TRAINER.VPOUR.PROMPT_SIZE = 30\n",
    "    cfg.TRAINER.VPOUR.SPSA_PARAMS = [0.0, 0.001, 40.0, 0.6, 0.1]\n",
    "    cfg.TRAINER.VPOUR.OPT_TYPE = \"spsa-gc\"\n",
    "    cfg.TRAINER.VPOUR.MOMS = 0.9\n",
    "    cfg.TRAINER.VPOUR.SP_AVG = 5\n",
    "\n",
    "    # ! BlackVIP\n",
    "    cfg.TRAINER.BLACKVIP = CN()\n",
    "    cfg.TRAINER.BLACKVIP.METHOD = 'coordinator'\n",
    "    cfg.TRAINER.BLACKVIP.PT_BACKBONE = ptb  # vit-base / vit-mae-base\n",
    "    cfg.TRAINER.BLACKVIP.SRC_DIM = 1568  # 784 / 1568 / 3136 #? => only for pre-trained Enc\n",
    "    cfg.TRAINER.BLACKVIP.E_OUT_DIM = 0  # 64 / 128 / 256 #? => only for scratch Enc\n",
    "    cfg.TRAINER.BLACKVIP.SPSA_PARAMS = [1.0, 0.005, 0.01, 0.4, 0.2]\n",
    "    cfg.TRAINER.BLACKVIP.OPT_TYPE = \"spsa-gc\"  # [spsa, spsa-gc, naive]\n",
    "    cfg.TRAINER.BLACKVIP.MOMS = b1  # first moment scale.\n",
    "    cfg.TRAINER.BLACKVIP.SP_AVG = 5  # grad estimates averaging steps\n",
    "    cfg.TRAINER.BLACKVIP.P_EPS = p_eps  # prompt scale\n",
    "    cfg.TRAINER.BLACKVIP.smoothing = False\n",
    "    # ! Black-Box Adversarial Reprogramming (BAR)\n",
    "    cfg.TRAINER.BAR = CN()\n",
    "    cfg.TRAINER.BAR.METHOD = 'reprogramming'\n",
    "    cfg.TRAINER.BAR.LRS = [0.01, 0.0001]\n",
    "    cfg.TRAINER.BAR.FRAME_SIZE = 224\n",
    "    cfg.TRAINER.BAR.SMOOTH = 0.01\n",
    "    cfg.TRAINER.BAR.SIMGA = 1.0\n",
    "    cfg.TRAINER.BAR.SP_AVG = 5\n",
    "    cfg.TRAINER.BAR.FOCAL_G = 2.0\n",
    "\n",
    "    # ! Full Fine Tune / Linear Probe\n",
    "    cfg.TRAINER.FTCLIP = CN()\n",
    "    cfg.TRAINER.FTCLIP.METHOD = 'ft'  # 'ft', 'lp'\n",
    "\n",
    "    # ! CoOp, CoCoOp\n",
    "    cfg.TRAINER.COOP = CN()\n",
    "    cfg.TRAINER.COOP.N_CTX = 16  # number of context vectors\n",
    "    cfg.TRAINER.COOP.CSC = False  # class-specific context\n",
    "    cfg.TRAINER.COOP.CTX_INIT = \"\"  # initialization words\n",
    "    cfg.TRAINER.COOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.COOP.CLASS_TOKEN_POSITION = \"end\"  # 'middle' or 'end' or 'front'\n",
    "\n",
    "    #\n",
    "    cfg.OPTIM.MAX_EPOCH = 500\n",
    "    cfg.DATASET.NUM_SHOTS = 16\n",
    "    cfg.TRAIN.CHECKPOINT_FREQ = 500\n",
    "    cfg.TRAINER.BLACKVIP.P_EPS_number = PATCH_NUM\n",
    "\n",
    "    cfg.TRAINER.COCOOP = CN()\n",
    "    cfg.TRAINER.COCOOP.N_CTX = 16  # number of context vectors\n",
    "    cfg.TRAINER.COCOOP.CTX_INIT = \"\"  # initialization words\n",
    "    cfg.TRAINER.COCOOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "def reset_cfg(cfg, args):\n",
    "    if args.root:\n",
    "        cfg.DATASET.ROOT = args.root\n",
    "\n",
    "    if args.eval_only:\n",
    "        cfg.eval_only = 1\n",
    "    else:\n",
    "        cfg.eval_only = 0\n",
    "\n",
    "    if args.output_dir:\n",
    "        cfg.OUTPUT_DIR = args.output_dir\n",
    "\n",
    "    if args.resume:\n",
    "        cfg.RESUME = args.resume\n",
    "\n",
    "    if args.seed:\n",
    "        cfg.SEED = args.seed\n",
    "\n",
    "    if args.wb_method_name != 'no':\n",
    "        cfg.WB_METHOD_NAME = args.wb_method_name\n",
    "\n",
    "    if args.use_wandb:\n",
    "        cfg.use_wandb = 1\n",
    "    else:\n",
    "        cfg.use_wandb = 0\n",
    "\n",
    "    cfg.EVAL_MODE = 'best'\n",
    "\n",
    "    if args.source_domains:\n",
    "        cfg.DATASET.SOURCE_DOMAINS = args.source_domains\n",
    "\n",
    "    if args.target_domains:\n",
    "        cfg.DATASET.TARGET_DOMAINS = args.target_domains\n",
    "\n",
    "    if args.transforms:\n",
    "        cfg.INPUT.TRANSFORMS = args.transforms\n",
    "\n",
    "    if args.trainer:\n",
    "        cfg.TRAINER.NAME = args.trainer\n",
    "\n",
    "    if args.backbone:\n",
    "        cfg.MODEL.BACKBONE.NAME = args.backbone\n",
    "\n",
    "    if args.head:\n",
    "        cfg.MODEL.HEAD.NAME = args.head\n",
    "    if args.patch_size:\n",
    "        cfg.TRAINER.PATCH = args.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4add6b-76f9-4da3-9ecf-1ff468a1ab4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/BLACKVIP/vit_b16.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "output_dir: output/eurosat/BLACKVIP/vit-mae-base_vit_b16/shot16_ep5000_orignal/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1\n",
      "patch_size: 16\n",
      "randomize: 1\n",
      "resume: \n",
      "root: ./datasets\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: BLACKVIP\n",
      "transforms: None\n",
      "use_wandb: False\n",
      "wb_method_name: no\n",
      "wb_name: test\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 32\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 128\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  COLOUR_BIASED_MNIST:\n",
      "    RANDOMIZE: True\n",
      "    TEST_N_CONFUSING_LABELS: 9\n",
      "    TEST_RHO: 0.2\n",
      "    TRAIN_N_CONFUSING_LABELS: 9\n",
      "    TRAIN_RHO: 0.8\n",
      "    USE_TEST_AS_VAL: True\n",
      "  LOCMNIST:\n",
      "    F_SIZE: 4\n",
      "    R_SIZE: 1\n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: ./datasets\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "EVAL_MODE: best\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.5\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 1\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 100\n",
      "  WARMUP_MIN_LR: 0.001\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: linear\n",
      "  WEIGHT_DECAY: 0.0\n",
      "OUTPUT_DIR: output/eurosat/BLACKVIP/vit-mae-base_vit_b16/shot16_ep5000_orignal/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 500\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  BAR:\n",
      "    FOCAL_G: 2.0\n",
      "    FRAME_SIZE: 224\n",
      "    LRS: [0.01, 0.0001]\n",
      "    METHOD: reprogramming\n",
      "    SIMGA: 1.0\n",
      "    SMOOTH: 0.01\n",
      "    SP_AVG: 5\n",
      "  BLACKVIP:\n",
      "    E_OUT_DIM: 0\n",
      "    METHOD: coordinator\n",
      "    MOMS: 0.9\n",
      "    OPT_TYPE: spsa-gc\n",
      "    PT_BACKBONE: vit-mae-base\n",
      "    P_EPS: 1.0\n",
      "    P_EPS_number: 0\n",
      "    SPSA_PARAMS: [1.0, 0.01, 0.01, 0.4, 0.1]\n",
      "    SP_AVG: 5\n",
      "    SRC_DIM: 1568\n",
      "    smoothing: False\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: end\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  FTCLIP:\n",
      "    METHOD: ft\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: BLACKVIP\n",
      "  PATCH: 16\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "  VPOUR:\n",
      "    IMAGE_SIZE: 224\n",
      "    METHOD: padding\n",
      "    MOMS: 0.9\n",
      "    OPT_TYPE: spsa-gc\n",
      "    PROMPT_SIZE: 30\n",
      "    SPSA_PARAMS: [0.0, 0.001, 40.0, 0.6, 0.1]\n",
      "    SP_AVG: 5\n",
      "  VPWB:\n",
      "    IMAGE_SIZE: 224\n",
      "    METHOD: padding\n",
      "    PREC: amp\n",
      "    PROMPT_SIZE: 30\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "eval_only: 0\n",
      "use_wandb: 0\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.2.0+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 20.04.6 LTS (x86_64)\n",
      "GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: Could not collect\n",
      "Libc version: glibc-2.31\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)\n",
      "Python platform: Linux-5.15.0-92-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce GTX 1080 Ti\n",
      "GPU 1: NVIDIA GeForce GTX 1080 Ti\n",
      "GPU 2: NVIDIA GeForce GTX 1080 Ti\n",
      "GPU 3: NVIDIA TITAN X (Pascal)\n",
      "\n",
      "Nvidia driver version: 545.23.08\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                       x86_64\n",
      "CPU op-mode(s):                     32-bit, 64-bit\n",
      "Byte Order:                         Little Endian\n",
      "Address sizes:                      43 bits physical, 48 bits virtual\n",
      "CPU(s):                             32\n",
      "On-line CPU(s) list:                0-31\n",
      "Thread(s) per core:                 2\n",
      "Core(s) per socket:                 16\n",
      "Socket(s):                          1\n",
      "NUMA node(s):                       2\n",
      "Vendor ID:                          AuthenticAMD\n",
      "CPU family:                         23\n",
      "Model:                              1\n",
      "Model name:                         AMD Ryzen Threadripper 1950X 16-Core Processor\n",
      "Stepping:                           1\n",
      "Frequency boost:                    enabled\n",
      "CPU MHz:                            2848.909\n",
      "CPU max MHz:                        3400.0000\n",
      "CPU min MHz:                        2200.0000\n",
      "BogoMIPS:                           6799.13\n",
      "Virtualization:                     AMD-V\n",
      "L1d cache:                          512 KiB\n",
      "L1i cache:                          1 MiB\n",
      "L2 cache:                           8 MiB\n",
      "L3 cache:                           32 MiB\n",
      "NUMA node0 CPU(s):                  0-7,16-23\n",
      "NUMA node1 CPU(s):                  8-15,24-31\n",
      "Vulnerability Gather data sampling: Not affected\n",
      "Vulnerability Itlb multihit:        Not affected\n",
      "Vulnerability L1tf:                 Not affected\n",
      "Vulnerability Mds:                  Not affected\n",
      "Vulnerability Meltdown:             Not affected\n",
      "Vulnerability Mmio stale data:      Not affected\n",
      "Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT vulnerable\n",
      "Vulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\n",
      "Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\n",
      "Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\n",
      "Vulnerability Srbds:                Not affected\n",
      "Vulnerability Tsx async abort:      Not affected\n",
      "Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==2.2.0+cu118\n",
      "[pip3] torchaudio==2.2.0+cu118\n",
      "[pip3] torchvision==0.17.0+cu118\n",
      "[pip3] triton==2.2.0\n",
      "[conda] torch                     2.2.0+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.2.0+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.17.0+cu118             pypi_0    pypi\n",
      "[conda] triton                    2.2.0                    pypi_0    pypi\n",
      "        Pillow (10.1.0)\n",
      "\n",
      "Loading trainer: BLACKVIP\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /home/connor/BlackVIP/datasets/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /home/connor/BlackVIP/datasets/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  10\n",
      "# train_x  160\n",
      "# val      40\n",
      "# test     8,100\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Text Prompts: ['a centered satellite photo of Annual Crop Land.', 'a centered satellite photo of Forest.', 'a centered satellite photo of Herbaceous Vegetation Land.', 'a centered satellite photo of Highway or Road.', 'a centered satellite photo of Industrial Buildings.', 'a centered satellite photo of Pasture Land.', 'a centered satellite photo of Permanent Crop Land.', 'a centered satellite photo of Residential Buildings.', 'a centered satellite photo of River.', 'a centered satellite photo of Sea or Lake.']\n",
      "Loading evaluator: Classification\n",
      "Load output/eurosat/BLACKVIP/vit-mae-base_vit_b16/shot16_ep5000_orignal/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1/coordinator/model.pth.tar-5000 to coordinator (epoch=5000)\n"
     ]
    }
   ],
   "source": [
    "def setup_cfg(args):\n",
    "    cfg = get_cfg_default()\n",
    "    extend_cfg(cfg)\n",
    "\n",
    "    # 1. From the dataset config file\n",
    "    if args.dataset_config_file:\n",
    "        cfg.merge_from_file(args.dataset_config_file)\n",
    "\n",
    "    # 2. From the method config file\n",
    "    if args.config_file:\n",
    "        cfg.merge_from_file(args.config_file)\n",
    "\n",
    "    # 3. From input arguments\n",
    "    reset_cfg(cfg, args)\n",
    "\n",
    "    # 4. From optional input arguments\n",
    "    # print(args.opts)\n",
    "    # cfg.merge_from_list(args.opts)\n",
    "\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "    \n",
    "import argparse\n",
    "\n",
    "# Manually creating a Namespace object to simulate command-line arguments\n",
    "args = argparse.Namespace(\n",
    "    root='./datasets',\n",
    "    output_dir=f'output/eurosat/BLACKVIP/vit-mae-base_vit_b16/shot16_ep5000_orignal/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1',\n",
    "    # output_dir = f'output/eurosat/BLACKVIP/vit-mae-base_vit_b16/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4_jh/seed1',\n",
    "    # output_dir = f'output/eurosat/BLACKVIP/vit-mae-base_vit_b16_nt/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1',\n",
    "    # output_dir = f'output/eurosat/BLACKVIP_right_t2t/vit-mae-base_vit_b16_t2t/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.2_sa0.01_sc0.005_eps0.4/seed1',\n",
    "    # output_dir=f'output/colour_biased_mnist_easy/BLACKVIP/vit-mae-base_vit_b16_noaug/shot16_ep5000/spsa-gc_b10.9/a0.4_g0.1_sa0.01_sc0.01_eps1.0/seed1',\n",
    "    resume='',\n",
    "    seed=1,\n",
    "    source_domains=None,  # Update this as needed\n",
    "    target_domains=None,  # Update this as needed\n",
    "    transforms=None,  # Update this as needed\n",
    "    config_file='configs/trainers/BLACKVIP/vit_b16.yaml',\n",
    "    dataset_config_file='configs/datasets/eurosat.yaml',\n",
    "    trainer='BLACKVIP',\n",
    "    backbone='',  # Update this as needed\n",
    "    head='',  # Update this as needed\n",
    "    eval_only=False,\n",
    "    model_dir='',  # Update this as needed\n",
    "    load_epoch=None,  # Update this as needed\n",
    "    no_train=False,\n",
    "    use_wandb=False,\n",
    "    wb_name='test',\n",
    "    wb_method_name='no',\n",
    "    randomize=1,\n",
    "    patch_size = 16\n",
    "\n",
    "    # opts=None  # Update this as needed, if you need to pass any additional command line arguments\n",
    ")\n",
    "def print_args(args, cfg):\n",
    "    print(\"***************\")\n",
    "    print(\"** Arguments **\")\n",
    "    print(\"***************\")\n",
    "    optkeys = list(args.__dict__.keys())\n",
    "    optkeys.sort()\n",
    "    for key in optkeys:\n",
    "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
    "    print(\"************\")\n",
    "    print(\"** Config **\")\n",
    "    print(\"************\")\n",
    "    print(cfg)\n",
    "    \n",
    "cfg = setup_cfg(args)\n",
    "if cfg.SEED >= 0:\n",
    "    print(\"Setting fixed seed: {}\".format(cfg.SEED))\n",
    "    set_random_seed(cfg.SEED)\n",
    "setup_logger(cfg.OUTPUT_DIR)\n",
    "\n",
    "if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print_args(args, cfg)\n",
    "print(\"Collecting env info ...\")\n",
    "\n",
    "try:\n",
    "    print(\"** System info **\\n{}\\n\".format(collect_env_info()))  #\n",
    "except:\n",
    "    pass\n",
    "\n",
    "baseacc, newacc, all_best_acc = 0, 0, 0\n",
    "\n",
    "from my_dassl.data.transforms import INTERPOLATION_MODES, build_transform\n",
    "import torchvision.transforms as T\n",
    "interp_mode = INTERPOLATION_MODES[cfg.INPUT.INTERPOLATION]\n",
    "to_tensor = []\n",
    "to_tensor += [T.Resize(cfg.INPUT.SIZE, interpolation=interp_mode)]\n",
    "to_tensor += [T.ToTensor()]\n",
    "if \"normalize\" in cfg.INPUT.TRANSFORMS:\n",
    "    normalize = T.Normalize(\n",
    "        mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD\n",
    "    )\n",
    "    to_tensor += [normalize]\n",
    "tensor_tr = T.Compose(to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf23060-d40f-426a-8a27-918e2a243652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTForImageClassification: ['decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.mask_token', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/connor/.conda/envs/bl/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "trainer = build_trainer(cfg)\n",
    "trainer.load_model(trainer.output_dir, epoch=5000)\n",
    "dtype = trainer.model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af5c88b1-8354-42be-bc39-17a721280c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define function to extract patches and calculate norms\n",
    "def extract_patches_and_calculate_norms(tensor, patch_size=4, threshold=150, check=False):\n",
    "    patches = tensor.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.contiguous().view(1, 3, -1, patch_size, patch_size)\n",
    "    patches = patches.permute(2, 0, 1, 3, 4).reshape(-1, 3, patch_size, patch_size)\n",
    "    norm_img = torch.norm(patches, p=2, dim=(1, 2, 3))\n",
    "    indices_under_t = norm_img < threshold\n",
    "    norms_under_t = norm_img[indices_under_t]\n",
    "    if check:\n",
    "        return norms_under_t, indices_under_t, norm_img\n",
    "    return norms_under_t, indices_under_t\n",
    "\n",
    "# Function to plot norms under threshold\n",
    "def plot_norms_under_threshold(ax, norms, indices, title, patch_size=4):\n",
    "    norms_image = torch.full((224 // patch_size, 224 // patch_size), float('nan'))\n",
    "    norms_image[indices.view(224 // patch_size, 224 // patch_size)] = norms\n",
    "    cax = ax.imshow(norms_image.numpy(), cmap='viridis')\n",
    "    fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f476efc-d4a5-4b2b-808e-0465522b4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "trainer.set_model_mode(\"eval\")\n",
    "trainer.evaluator.reset()\n",
    "for ii,i in enumerate(glob('datasets/eurosat/2750/River/*.jpg')):\n",
    "    ori_img = Image.open(i).convert(\"RGB\")\n",
    "    img = tensor_tr(ori_img)\n",
    "    img = img.unsqueeze(0) \n",
    "    prompt, z = trainer.model.coordinator(img.cuda().type(dtype))\n",
    "    if ii == 150:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f036826-4e88-4890-902a-192889fff2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2813176"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.flatten().cpu().numpy().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68eded9e-e675-4356-b3f3-039080fd3aa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clip_clipping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create image tensor (assuming the image size is [1, 3, 224, 224])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m eps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.4\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m input_tensor_1 \u001b[38;5;241m=\u001b[39m \u001b[43mclip_clipping\u001b[49m(img\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;241m+\u001b[39m prompt\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;241m*\u001b[39m eps[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m input_tensor_2 \u001b[38;5;241m=\u001b[39m clip_clipping(img\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;241m+\u001b[39m prompt\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;241m*\u001b[39m eps[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip_clipping' is not defined"
     ]
    }
   ],
   "source": [
    "# Create image tensor (assuming the image size is [1, 3, 224, 224])\n",
    "eps = [1, 0.4]\n",
    "input_tensor_1 = clip_clipping(img.cpu() + prompt.cpu() * eps[0])\n",
    "input_tensor_2 = clip_clipping(img.cpu() + prompt.cpu() * eps[1])\n",
    "patch_size = 4\n",
    "threshold = 180\n",
    "\n",
    "# Process both input and prompt tensors\n",
    "norms_img, img_indices = extract_patches_and_calculate_norms(img, patch_size, threshold)\n",
    "norms_prompt, prompt_indices = extract_patches_and_calculate_norms(clip_clipping(prompt.cpu()), patch_size, threshold)\n",
    "norms_input_1, input_1_indices = extract_patches_and_calculate_norms(input_tensor_1, patch_size, threshold)\n",
    "norms_input_2, input_2_indices = extract_patches_and_calculate_norms(input_tensor_2, patch_size, threshold)\n",
    "\n",
    "# Setup plot\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# Original image\n",
    "axs[0, 0].imshow(ori_img.resize((224, 224)))\n",
    "axs[0, 0].set_title('Original Image')\n",
    "\n",
    "# Plot norms under threshold for input and prompt\n",
    "plot_norms_under_threshold(axs[0, 1], norms_img, img_indices, f'Input Norms < {threshold}')\n",
    "plot_norms_under_threshold(axs[1, 0], norms_prompt, prompt_indices, f'Prompt Norms ')\n",
    "plot_norms_under_threshold(axs[1, 1], norms_input_1, input_1_indices, f'Prompt Norms, eps : {eps[0]}')\n",
    "plot_norms_under_threshold(axs[1, 2], norms_input_2, input_2_indices, f'Prompt Norms, eps : {eps[1]}')\n",
    "\n",
    "# Distribution of all norms (Histogram)\n",
    "axs[0, 2].hist(clip_clipping(prompt).flatten().cpu().numpy(), bins=30, alpha=0.7, log=True)\n",
    "axs[0, 2].set_xlabel('Norm Value')\n",
    "axs[0, 2].set_ylabel('Frequency (Log Scale)')\n",
    "axs[0, 2].set_title('Distribution of All Norms')\n",
    "\n",
    "plt.tight_layout()\n",
    "# path = f'check_prompt/{eps}_{patch_size}_{check_under}'\n",
    "# os.makedirs(path,exist_ok=True)\n",
    "# plt.savefig(f'{path}/{os.path.basename(i)}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450aa0a-10c2-40c7-a58d-a6a4f955be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_clipping(x):\n",
    "    #! -inf ~ inf -> CLIP's input RGB range\n",
    "    if len(x.shape) == 3:\n",
    "        out = torch.cat([torch.clip(x[0,:,:], min=-1.79226253, max=1.93033625).unsqueeze(0),\n",
    "                     torch.clip(x[1,:,:], min=-1.75209713, max=2.07488384).unsqueeze(0),\n",
    "                     torch.clip(x[2,:,:], min=-1.48021977, max=2.14589699).unsqueeze(0)], dim=0)\n",
    "    else:\n",
    "        out = torch.cat([torch.clip(x[:,0,:,:], min=-1.79226253, max=1.93033625).unsqueeze(1),\n",
    "                        torch.clip(x[:,1,:,:], min=-1.75209713, max=2.07488384).unsqueeze(1),\n",
    "                        torch.clip(x[:,2,:,:], min=-1.48021977, max=2.14589699).unsqueeze(1)], dim=1)\n",
    "    return out\n",
    "trainer.set_model_mode(\"eval\")\n",
    "trainer.evaluator.reset()\n",
    "data_loader =trainer.test_loader\n",
    "\n",
    "from tqdm import tqdm\n",
    "whi = 1\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "    if batch_idx == whi :\n",
    "        input, label = trainer.parse_batch_test(batch)\n",
    "        prompt, z = trainer.model.coordinator(input.cuda().type(dtype))\n",
    "        prompt = clip_clipping(prompt)\n",
    "        paths = batch['impath']\n",
    "    elif batch_idx > whi :\n",
    "        break\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate a dummy tensor representing a batch of 16 images, each with 3 color channels and 224x224 pixels\n",
    "# In a real scenario, this tensor would be your input images\n",
    "\n",
    "\n",
    "# Flatten the images to shape [32, 150528]\n",
    "flattened_images = prompt.cpu().view(32, -1)\n",
    "flattened_images[1] = flattened_images[1]\n",
    "# Compute the cosine similarity\n",
    "cos_sim_matrix = torch.nn.functional.cosine_similarity(flattened_images[:, None], flattened_images, dim=-1)\n",
    "\n",
    "# 유클리드 거리 계산 함수\n",
    "def euclidean_distance(x, y):\n",
    "    return torch.sqrt(torch.sum((x - y) ** 2, dim=-1))\n",
    "\n",
    "# 맨해튼 거리 계산 함수\n",
    "def manhattan_distance(x, y):\n",
    "    return torch.sum(torch.abs(x - y), dim=-1)\n",
    "\n",
    "euclidean_dist_matrix = torch.zeros(flattened_images.size(0), flattened_images.size(0))\n",
    "manhattan_dist_matrix = torch.zeros(flattened_images.size(0), flattened_images.size(0))\n",
    "\n",
    "for i in range(flattened_images.size(0)):\n",
    "    for j in range(flattened_images.size(0)):\n",
    "        euclidean_dist_matrix[i, j] = euclidean_distance(flattened_images[i], flattened_images[j])\n",
    "        manhattan_dist_matrix[i, j] = manhattan_distance(flattened_images[i], flattened_images[j])\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 24))\n",
    "\n",
    "# 코사인 유사도 매트릭스\n",
    "sns.heatmap(cos_sim_matrix.numpy(), ax=axes[0], annot=False, cmap='coolwarm')\n",
    "axes[0].set_title(\"Cosine Similarity Matrix\")\n",
    "axes[0].set_xlabel(\"Image Index\")\n",
    "axes[0].set_ylabel(\"Image Index\")\n",
    "\n",
    "# 유클리드 거리 매트릭스\n",
    "sns.heatmap(euclidean_dist_matrix.numpy(), ax=axes[1], annot=False, cmap='coolwarm')\n",
    "axes[1].set_title(\"Euclidean Distance Matrix\")\n",
    "axes[1].set_xlabel(\"Image Index\")\n",
    "axes[1].set_ylabel(\"Image Index\")\n",
    "\n",
    "# 맨해튼 거리 매트릭스\n",
    "sns.heatmap(manhattan_dist_matrix.numpy(), ax=axes[2], annot=False, cmap='coolwarm')\n",
    "axes[2].set_title(\"Manhattan Distance Matrix\")\n",
    "axes[2].set_xlabel(\"Image Index\")\n",
    "axes[2].set_ylabel(\"Image Index\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bf897-878b-4744-a2db-267d6b703950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600f373-1767-4150-89f6-0115f7b6ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_config_file = f'configs/datasets/eurosat.yaml'\n",
    "cfg = setup_cfg(args)\n",
    "# for strage test\n",
    "AVAI_CHOICES = [\"random_flip\", \"random_resized_crop\", \"normalize\", \"instance_norm\", \"random_crop\", \"random_translation\", \"center_crop\",\n",
    "    \"cutout\", \"imagenet_policy\", \"cifar10_policy\", \"svhn_policy\", \"randaugment\", \"randaugment_fixmatch\", \"randaugment2\", \"gaussian_noise\",\n",
    "    \"colorjitter\", \"randomgrayscale\", \"gaussian_blur\",]\n",
    "new_test_type = ['randaugment_fixmatch'] # None # ['randaugment_fixmatch'] # None\n",
    "# data suffle\n",
    "cfg.merge_from_list(['DATALOADER.TEST.SAMPLER', 'SequentialSampler']) # 'RandomSampler', 'SequentialSampler'\n",
    "# for other data\n",
    "cfg.merge_from_file(args.dataset_config_file)\n",
    "\n",
    "if new_test_type is None:\n",
    "    pass\n",
    "elif new_test_type[0] == 'base':\n",
    "    cfg.merge_from_list(['INPUT.TRANSFORMS', ('normalize', 'new_test_type')])\n",
    "else:\n",
    "    cfg.merge_from_list(['INPUT.TRANSFORMS', ('normalize', *new_test_type, 'new_test_type')])\n",
    "\n",
    "# Creating the tuple with the desired format\n",
    "trainer = build_trainer(cfg)\n",
    "trainer.load_model(trainer.output_dir, epoch=5000)\n",
    "dtype = trainer.model.dtype\n",
    "data_loader =trainer.test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b58715-eb06-42fb-8be7-c4de3609d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_clipping(x):\n",
    "    #! -inf ~ inf -> CLIP's input RGB range\n",
    "    if len(x.shape) == 3:\n",
    "        out = torch.cat([torch.clip(x[0,:,:], min=-1.79226253, max=1.93033625).unsqueeze(0),\n",
    "                     torch.clip(x[1,:,:], min=-1.75209713, max=2.07488384).unsqueeze(0),\n",
    "                     torch.clip(x[2,:,:], min=-1.48021977, max=2.14589699).unsqueeze(0)], dim=0)\n",
    "    else:\n",
    "        out = torch.cat([torch.clip(x[:,0,:,:], min=-1.79226253, max=1.93033625).unsqueeze(1),\n",
    "                        torch.clip(x[:,1,:,:], min=-1.75209713, max=2.07488384).unsqueeze(1),\n",
    "                        torch.clip(x[:,2,:,:], min=-1.48021977, max=2.14589699).unsqueeze(1)], dim=1)\n",
    "    return out\n",
    "trainer.set_model_mode(\"eval\")\n",
    "trainer.evaluator.reset()\n",
    "data_loader =trainer.test_loader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "cnt = 0\n",
    "for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "    input, label = trainer.parse_batch_test(batch)\n",
    "    # if label[0] == 8:\n",
    "    if not torch.all(label == label[0]):\n",
    "        continue\n",
    "    cnt +=1\n",
    "    prompt, z = trainer.model.coordinator(input.cuda().type(dtype))\n",
    "    prompt = clip_clipping(prompt)\n",
    "    # Flatten the images to shape [32, 150528]\n",
    "    flattened_images = prompt.cpu().view(32, -1)\n",
    "    flattened_images[1] = flattened_images[1]\n",
    "    # Compute the cosine similarity\n",
    "    cos_sim_matrix = torch.nn.functional.cosine_similarity(flattened_images[:, None], flattened_images, dim=-1)\n",
    "    mask = np.triu_indices_from(cos_sim_matrix, k=1)\n",
    "    if not batch_idx:\n",
    "        cos_sim_values = cos_sim_matrix[mask]\n",
    "    else:\n",
    "        cos_sim_values = torch.cat((cos_sim_values, cos_sim_matrix[mask]), dim=0)\n",
    "    if cnt>150:\n",
    "        break\n",
    "# Step 3 remains the same: Plot the distribution of cosine similarity values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cos_sim_values, bins=30, kde=0)\n",
    "try: \n",
    "    plt.title(f'Distribution of Cosine Similarities_{new_test_type[0]} (Excluding Self-Comparison)')\n",
    "except TypeError:\n",
    "    pass\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0.88, 0.99)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98b2ea-c375-43cf-a559-1c865d6d9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.std(cos_sim_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7198c-b220-4300-bc60-cc2e6d3abdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(cos_sim_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05c805-b108-4ce8-a013-5214b69bb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "    input, label = trainer.parse_batch_test(batch)\n",
    "    image_tensor = input[0].cpu()\n",
    "\n",
    "    # PyTorch tensors for images are in the format (C, H, W) but Matplotlib expects (H, W, C)\n",
    "    # So, we need to convert the format\n",
    "    image = image_tensor.permute(1, 2, 0)\n",
    "    \n",
    "    # The values might also need to be scaled to the range [0, 1] if they aren't already\n",
    "    image = image.numpy() # Convert to numpy array\n",
    "    image = (image - image.min()) / (image.max() - image.min()) # Scale to [0, 1]\n",
    "    if batch_idx % 2:\n",
    "        # Plotting the image\n",
    "        plt.title(f'{batch_idx},{label[0]}')\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off') # Hide axis\n",
    "        plt.show()\n",
    "    if batch_idx == 80:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1aaa20-b8d1-4d03-a62b-6bf20ce58f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'batch_images' is your tensor with shape (32, 3, 224, 224)\n",
    "# Selecting the first image from the batch\n",
    "image_tensor = input[0].cpu()\n",
    "\n",
    "# PyTorch tensors for images are in the format (C, H, W) but Matplotlib expects (H, W, C)\n",
    "# So, we need to convert the format\n",
    "image = image_tensor.permute(1, 2, 0)\n",
    "\n",
    "# The values might also need to be scaled to the range [0, 1] if they aren't already\n",
    "image = image.numpy() # Convert to numpy array\n",
    "image = (image - image.min()) / (image.max() - image.min()) # Scale to [0, 1]\n",
    "\n",
    "# Plotting the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off') # Hide axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1e6ad-c41d-4189-80bd-54402af43ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "    input, label = trainer.parse_batch_test(batch)\n",
    "    image_tensor = input[0].cpu()\n",
    "\n",
    "    # PyTorch tensors for images are in the format (C, H, W) but Matplotlib expects (H, W, C)\n",
    "    # So, we need to convert the format\n",
    "    image = image_tensor.permute(1, 2, 0)\n",
    "    \n",
    "    # The values might also need to be scaled to the range [0, 1] if they aren't already\n",
    "    image = image.numpy() # Convert to numpy array\n",
    "    image = (image - image.min()) / (image.max() - image.min()) # Scale to [0, 1]\n",
    "    if batch_idx % 2:\n",
    "        # Plotting the image\n",
    "        plt.title(f'{batch_idx},{label[0]}')\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off') # Hide axis\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f16cf-c565-4bdc-84d5-050b13ac21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_config_file = f'configs/datasets/eurosat.yaml'\n",
    "cfg = setup_cfg(args)\n",
    "# for strage test\n",
    "AVAI_CHOICES = [\"random_flip\", \"random_resized_crop\", \"normalize\", \"instance_norm\", \"random_crop\", \"random_translation\", \"center_crop\",\n",
    "    \"cutout\", \"imagenet_policy\", \"cifar10_policy\", \"svhn_policy\", \"randaugment\", \"randaugment_fixmatch\", \"randaugment2\", \"gaussian_noise\",\n",
    "    \"colorjitter\", \"randomgrayscale\", \"gaussian_blur\",]\n",
    "new_test_type = None # ['random_resized_crop'] # None\n",
    "# data suffle\n",
    "cfg.merge_from_list(['DATALOADER.TEST.SAMPLER', 'RandomSampler']) # 'RandomSampler', 'SequentialSampler'\n",
    "# for other data\n",
    "cfg.merge_from_file(args.dataset_config_file)\n",
    "\n",
    "if new_test_type is None:\n",
    "    pass\n",
    "elif new_test_type[0] == 'base':\n",
    "    cfg.merge_from_list(['INPUT.TRANSFORMS', ('normalize', 'new_test_type')])\n",
    "else:\n",
    "    cfg.merge_from_list(['INPUT.TRANSFORMS', ('normalize', *new_test_type, 'new_test_type')])\n",
    "\n",
    "# Creating the tuple with the desired format\n",
    "trainer1 = build_trainer(cfg)\n",
    "trainer1.load_model(trainer.output_dir, epoch=5000)\n",
    "dtype = trainer1.model.dtype\n",
    "data_loader1 =trainer1.test_loader\n",
    "\n",
    "list_dataset = ['resisc45','caltech101', 'clevr', 'dtd.yaml','food101','stanford_cars','sun397','ucf101','resisc45','oxford_flowers']\n",
    "args.dataset_config_file = f'configs/datasets/{list_dataset[0]}.yaml'\n",
    "cfg = setup_cfg(args)\n",
    "# for strage test\n",
    "AVAI_CHOICES = [\"random_flip\", \"random_resized_crop\", \"normalize\", \"instance_norm\", \"random_crop\", \"random_translation\", \"center_crop\",\n",
    "    \"cutout\", \"imagenet_policy\", \"cifar10_policy\", \"svhn_policy\", \"randaugment\", \"randaugment_fixmatch\", \"randaugment2\", \"gaussian_noise\",\n",
    "    \"colorjitter\", \"randomgrayscale\", \"gaussian_blur\",]\n",
    "new_test_type = None # ['random_resized_crop'] # None\n",
    "# data suffle\n",
    "cfg.merge_from_list(['DATALOADER.TEST.SAMPLER', 'RandomSampler']) # 'RandomSampler', 'SequentialSampler'\n",
    "# for other data\n",
    "cfg.merge_from_file(args.dataset_config_file)\n",
    "\n",
    "if new_test_type is None:\n",
    "    pass\n",
    "elif new_test_type[0] == 'base':\n",
    "    cfg.merge_from_list(['INPUT.TRANSFORMS', ('normalize', 'new_test_type')])\n",
    "else:\n",
    "    cfg.merge_from_list(['INPUT.TRANSFORMS', ('normalize', *new_test_type, 'new_test_type')])\n",
    "\n",
    "# Creating the tuple with the desired format\n",
    "trainer2 = build_trainer(cfg)\n",
    "trainer2.load_model(trainer.output_dir, epoch=5000)\n",
    "dtype = trainer2.model.dtype\n",
    "data_loader2 =trainer2.test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efda2fe-4ee1-420b-ad97-09261e5adb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_clipping(x):\n",
    "    #! -inf ~ inf -> CLIP's input RGB range\n",
    "    if len(x.shape) == 3:\n",
    "        out = torch.cat([torch.clip(x[0,:,:], min=-1.79226253, max=1.93033625).unsqueeze(0),\n",
    "                     torch.clip(x[1,:,:], min=-1.75209713, max=2.07488384).unsqueeze(0),\n",
    "                     torch.clip(x[2,:,:], min=-1.48021977, max=2.14589699).unsqueeze(0)], dim=0)\n",
    "    else:\n",
    "        out = torch.cat([torch.clip(x[:,0,:,:], min=-1.79226253, max=1.93033625).unsqueeze(1),\n",
    "                        torch.clip(x[:,1,:,:], min=-1.75209713, max=2.07488384).unsqueeze(1),\n",
    "                        torch.clip(x[:,2,:,:], min=-1.48021977, max=2.14589699).unsqueeze(1)], dim=1)\n",
    "    return out\n",
    "trainer1.set_model_mode(\"eval\")\n",
    "trainer1.evaluator.reset()\n",
    "trainer2.set_model_mode(\"eval\")\n",
    "trainer2.evaluator.reset()\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "cnt = 0\n",
    "for batch_idx, (batch1,batch2) in enumerate(tqdm(zip(data_loader1,data_loader2))):\n",
    "    input1, label = trainer.parse_batch_test(batch1)\n",
    "    input2, label = trainer.parse_batch_test(batch2)\n",
    "    # if label[0] == 8:\n",
    "    # if not torch.all(label == label[0]):\n",
    "    #     continue\n",
    "    cnt +=1\n",
    "    prompt1, z = trainer1.model.coordinator(input1.cuda().type(dtype))\n",
    "    prompt2, z = trainer2.model.coordinator(input2.cuda().type(dtype))\n",
    "    prompt1 = clip_clipping(prompt1)\n",
    "    prompt2 = clip_clipping(prompt2)\n",
    "    # Flatten the images to shape [32, 150528]\n",
    "    flattened_images1 = prompt1.cpu().view(32, -1)\n",
    "    flattened_images2 = prompt2.cpu().view(32, -1)\n",
    "    # flattened_images = torch.concat((flattened_images1[:16],flattened_images2[:16]),dim=0)\n",
    "    # Compute the cosine similarity\n",
    "    cos_sim_matrix = torch.nn.functional.cosine_similarity(flattened_images1[:, None], flattened_images2, dim=-1)\n",
    "    # mask = np.triu_indices_from(cos_sim_matrix, k=1)\n",
    "    if not batch_idx:\n",
    "        cos_sim_values = cos_sim_matrix.flatten()\n",
    "    else:\n",
    "        cos_sim_values = torch.cat((cos_sim_values, cos_sim_matrix.flatten()), dim=0)\n",
    "    if cnt>70:\n",
    "        break\n",
    "# Step 3 remains the same: Plot the distribution of cosine similarity values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(cos_sim_values, bins=30, kde=0)\n",
    "try: \n",
    "    plt.title(f'Distribution of Cosine Similarities_{new_test_type[0]} (Excluding Self-Comparison)')\n",
    "except TypeError:\n",
    "    pass\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0.88, 0.99)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c99270-166d-4c01-b71a-ccc9f3895122",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(cos_sim_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14cebf0-47ce-4e6f-8437-b1c08bb63e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.std(cos_sim_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91fc0b-d6c5-4b34-a6ca-baa1d6f8c9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15e0ef83-bf20-4256-9526-bd6695c30597",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m24\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 코사인 유사도 매트릭스\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mheatmap(cos_sim_matrix\u001b[38;5;241m.\u001b[39mnumpy(), ax\u001b[38;5;241m=\u001b[39maxes[\u001b[38;5;241m0\u001b[39m], annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#, vmin=0.97)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCosine Similarity Matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage Index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x2400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(10, 24))\n",
    "\n",
    "# 코사인 유사도 매트릭스\n",
    "sns.heatmap(cos_sim_matrix.numpy(), ax=axes[0], annot=False, cmap='coolwarm')#, vmin=0.97)\n",
    "axes[0].set_title(\"Cosine Similarity Matrix\")\n",
    "axes[0].set_xlabel(\"Image Index\")\n",
    "axes[0].set_ylabel(\"Image Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e0faa-b1ff-4e5a-b37c-bfab6b1c02cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bl",
   "language": "python",
   "name": "bl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
